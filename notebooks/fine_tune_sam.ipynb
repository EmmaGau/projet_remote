{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Training a prompt model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmagauillard/Documents/MVA/Remote_sensing/projet/projet_remote/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import torch\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,  DataLoader\n",
    "from PIL import Image\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from transformers import SamModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"vit_h\"\n",
    "path_to_checkpoint = '../checkpoint/sam_vit_h_4b8939.pth'\n",
    "\n",
    "# Loading the model\n",
    "sam_model = sam_model_registry[model_type](checkpoint=path_to_checkpoint)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../split\"\n",
    "\n",
    "train_img_dir = os.path.join(data_dir, 'train', 'img')\n",
    "test_img_dir = os.path.join(data_dir, 'val', 'img')\n",
    "train_msk_dir = os.path.join(data_dir, 'train', 'msk')\n",
    "test_msk_dir = os.path.join(data_dir, 'val', 'msk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_equalization_8bit(im, percentiles=5):\n",
    "    ''' im is a numpy array\n",
    "        returns a numpy array\n",
    "    '''\n",
    "    out = np.zeros_like(im)\n",
    "    # faire l'equalization par channel\n",
    "    def equalize(im_channel):\n",
    "        v_min, v_max = np.percentile(im_channel,percentiles),np.percentile(im_channel, 100 - percentiles)\n",
    "\n",
    "        # Clip the image to the percentile values\n",
    "        im_clipped = np.clip(im_channel, v_min, v_max)\n",
    "\n",
    "        # Scale the image to the 0-255 range\n",
    "        im_scaled = np.round((im_clipped - v_min) / (v_max - v_min))\n",
    "        return im_scaled.astype(np.uint8)\n",
    "    \n",
    "    for channel in range(im.shape[0]):\n",
    "        out[channel,:,:] = equalize(im[channel,:,:])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S1S2Dataset(Dataset):\n",
    "    def __init__(self, img_folder, mask_folder, processor, transform=None, target_transform=None,):\n",
    "        self.img_folder = img_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.img_filenames = [f for f in os.listdir(img_folder) if f.endswith('.tif') and \"img\" in f]\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        filename = self.img_filenames[idx]\n",
    "        img_path = os.path.join(self.img_folder, filename)\n",
    "        \n",
    "        # image\n",
    "        with rasterio.open(img_path) as src:\n",
    "            image = src.read().astype(np.float32)[0:3, :, :]\n",
    "            image = simple_equalization_8bit(image, percentiles=5) \n",
    "            image = torch.from_numpy(image) # shape (C, H, W)\n",
    "            \n",
    "            image = self.processor(image, return_tensors=\"pt\")\n",
    "            # remove batch dimension\n",
    "            image = {k: v.squeeze(0) for k, v in image.items()}\n",
    "\n",
    "        # masque\n",
    "        mask_filename = filename.replace(\"img\", \"msk\")\n",
    "        mask_path = os.path.join(self.mask_folder, mask_filename)\n",
    "        with rasterio.open(mask_path) as src:\n",
    "            mask = src.read()[0].astype(np.float32)\n",
    "            mask = torch.from_numpy(mask)\n",
    "            mask = mask.unsqueeze(0)  # Ajouter une dimension de canal (C, H, W)\n",
    "\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        #     mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "transform= transforms.Compose([\n",
    "    transforms.Resize((1024, 1024)), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_target = transforms.Compose([transforms.Resize((224, 224))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamProcessor\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "dataset = S1S2Dataset(train_img_dir, train_msk_dir, transform=transform, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmagauillard/Documents/MVA/Remote_sensing/projet/projet_remote/env/lib/python3.11/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader: 3290\n",
      "torch.Size([4, 3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(\"train_loader:\", len(train_loader))\n",
    "print(next(iter(train_loader))[0][\"pixel_values\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.mask_decoder.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm.tqdm(enumerate(train_loader, 0)):\n",
    "        inputs, labels = data\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        outputs = model(pixel_values=inputs[\"pixel_values\"].to(device), multimask_output=False)\n",
    "        \n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        print(' predicted_masks',predicted_masks.shape)\n",
    "        print('labels', labels.shape)\n",
    "        \n",
    "        loss = criterion(predicted_masks, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        print(loss.item())\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "# torch.save(model.state_dict(), PATH)\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
